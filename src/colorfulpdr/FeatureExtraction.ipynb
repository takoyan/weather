{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob(\"../../etc/features/*.csv\"):\n",
    "    print(os.path.basename(file).split(\".csv\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benri Kansu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合成加速度\n",
    "def get_SynAcc(x, y, z):\n",
    "    return math.sqrt(x*x + y*y + z*z)\n",
    "\n",
    "# 平均\n",
    "def get_ave(df):\n",
    "    stack_list=[]\n",
    "    for row in df.itertuples():\n",
    "        x, y, z=row[1], row[2], row[3]\n",
    "        stack_list.append(get_SynAcc(x, y, z))\n",
    "    return sum(stack_list)/len(stack_list)\n",
    "\n",
    "# 各軸の平均\n",
    "def get_Ave_value(value):\n",
    "    return sum(value)/len(value)\n",
    "\n",
    "# 各軸の標準偏差\n",
    "def get_Std_value(value):\n",
    "    stack_list=[]\n",
    "    ave=get_Ave_value(value)\n",
    "    stack_list=[(v-ave)**2 for v in value]\n",
    "    return math.sqrt(sum(stack_list)/(len(value)-1))\n",
    "\n",
    "# 幅\n",
    "def get_Range(df):\n",
    "    stack_list=[]\n",
    "    for row in df.itertuples():\n",
    "        x, y, z=row[1], row[2], row[3]\n",
    "        stack_list.append(get_SynAcc(x, y, z))\n",
    "    return max(stack_list) - min(stack_list)\n",
    "\n",
    "# 標準偏差\n",
    "def get_Std(df):\n",
    "    stack_list=[]\n",
    "    ave=get_ave(df)\n",
    "    for row in df.itertuples():\n",
    "        syn=get_SynAcc(row[1], row[2], row[3])\n",
    "        stack_list.append((syn-ave)**2)\n",
    "    return math.sqrt(sum(stack_list)/(len(df)-1))\n",
    "\n",
    "# 歪度\n",
    "def get_Skewness(df):\n",
    "    stack_list=[]\n",
    "    ave=get_ave(df)\n",
    "    for row in df.itertuples():\n",
    "        syn=get_SynAcc(row[1], row[2], row[3])\n",
    "        stack_list.append((syn-ave)**3)\n",
    "    return sum(stack_list)/(get_Std(df)**3)*(len(df)/((len(df)-1)*(len(df)-2)))\n",
    "\n",
    "# 尖度\n",
    "def get_Kurtosis(df):\n",
    "    stack_list=[]\n",
    "    ave=get_ave(df)\n",
    "    for row in df.itertuples():\n",
    "        syn=get_SynAcc(row[1], row[2], row[3])\n",
    "        stack_list.append((syn-ave)**4)\n",
    "    return ((sum(stack_list)/(get_Std(df)**4))*((len(df)*(len(df)+1))/((len(df)-1)*(len(df)-2)*(len(df)-3)))) - (3*(len(df)-1)**2)/((len(df)-2)*(len(df)-3))\n",
    "\n",
    "# エネルギー\n",
    "def get_Energy(df):\n",
    "    stack_list=[]\n",
    "    for row in df.itertuples():\n",
    "        x, y, z=row[1], row[2], row[3]\n",
    "        stack_list.append(get_SynAcc(x, y, z)**2)\n",
    "    return sum(stack_list)\n",
    "\n",
    "def get_Fft(df):\n",
    "    stack_list=[]\n",
    "    for row in df.itertuples():\n",
    "        x,y,z=row[1], row[2], row[3]\n",
    "        stack_list.append(get_SynAcc(x,y,z))\n",
    "    return max(np.fft.fft(stack_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"../../etc/no_header_pdr_raw_data/{}.json\".format('5NM1shibataku'), orient='records', lines=True)\n",
    "df=df[df['type']=='Accelerometer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# タイムウインド重複なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"../../etc/no_header_pdr_raw_data/{}.json\".format(\"7NM8miyazaki\"), orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUND=1000\n",
    "\n",
    "df_all=df[df['unixTime']//ROUND==((df['unixTime'].min()//ROUND)+30)]\n",
    "df_acc=df_all[df_all['type']=='Accelerometer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_list=[]\n",
    "for row in df_acc.itertuples():\n",
    "    x,y,z=row[1], row[2], row[3]\n",
    "    stack_list.append(get_SynAcc(x,y,z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fft(time, ampl):\n",
    "   \n",
    "    # サンプリング周期[sec]の計算 #################################################\n",
    "    sampling_cycle = time\n",
    "   \n",
    "    # データ数カウント ############################################################\n",
    "    N = len(ampl)\n",
    "   \n",
    "    # 高速フーリエ変換（ FFT ） ####################################################\n",
    "    fft_ampl = np.fft.fft(ampl)\n",
    "   \n",
    "    # FFT の複素数結果を絶対に変換 ###############################################\n",
    "    abs_fft_amp = np.abs(fft_ampl)\n",
    "   \n",
    "    # 振幅をもとの信号に揃える #####################################################\n",
    "    abs_fft_amp    = abs_fft_amp    / N * 2   # 交流成分\n",
    "    abs_fft_amp[0] = abs_fft_amp[0] / 2       # 直流成分\n",
    "   \n",
    "    # 周波数軸のデータ作成 #######################################################\n",
    "    frequency = np.linspace(0, 1.0/sampling_cycle, N) # 周波数軸　linspace(開始, 終了, 分割数)\n",
    "   \n",
    "    # ピーク検出 ################################################################\n",
    "    maximal_idx = signal.argrelmax(abs_fft_amp[:int(N/2)+1], order=1) # 極大値インデックスの取得\n",
    "    print(maximal_idx)\n",
    "    print(abs_fft_amp)\n",
    "   \n",
    "    # グラフ表示 ################################################################\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(frequency[:int(N/2)+1], abs_fft_amp[:int(N/2)+1])\n",
    "    plt.scatter(frequency[maximal_idx], abs_fft_amp[maximal_idx], c='red', s=25)\n",
    "    plt.grid(True)\n",
    "    plt.title('Fast Fourier Transform')\n",
    "    plt.xlabel('freqency[Hz]')\n",
    "    plt.ylabel('amplitude')\n",
    "    \n",
    "    return fft_ampl\n",
    "    \n",
    "def low_pass_filter(fft_time, fft_amp, cut_off):\n",
    "   \n",
    "    # データ数カウント ############################################################\n",
    "    N = len(fft_amp)\n",
    "   \n",
    "    # cut_off 以下の周波数の amplitude をゼロにする ################################\n",
    "    cut_off2 = fft_time - cut_off\n",
    "    fft_amp[((fft_time > cut_off)&(fft_time < cut_off2))] = 0 + 0j\n",
    "          \n",
    "    # グラフ用に実波形データに変換 #################################################\n",
    "   \n",
    "    # FFT の複素数結果を絶対に変換\n",
    "    abs_fft_amp = np.abs(fft_amp)\n",
    "   \n",
    "    # 振幅をもとの信号に揃える\n",
    "    abs_fft_amp    = abs_fft_amp    / N * 2 # 交流成分\n",
    "    abs_fft_amp[0] = abs_fft_amp[0] / 2     # 直流成分\n",
    "   \n",
    "    # ピーク検出 ################################################################\n",
    "    #maximal_idx = signal.argrelmax(abs_fft_amp[:int(N/2)+1], order=1) # 極大値インデックスの取得\n",
    "    maximal_idx = signal.argrelmax(abs_fft_amp, order=1) # 極大値インデックスの取得\n",
    "   \n",
    "    # グラフ表示 ################################################################\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    #plt.plot(fft_time[:int(N/2)+1], abs_fft_amp[:int(N/2)+1])\n",
    "    #plt.scatter(fft_time[maximal_idx], abs_fft_amp[maximal_idx], c='red', s=25)\n",
    "    plt.plot(fft_time, abs_fft_amp)\n",
    "    plt.scatter(fft_time[maximal_idx], abs_fft_amp[maximal_idx], c='red', s=25)\n",
    "    plt.grid(True)\n",
    "    plt.title('Low Pass Filter')\n",
    "    plt.xlabel('freqency[Hz]')\n",
    "    plt.ylabel('amplitude')\n",
    "   \n",
    "    return fft_time, fft_amp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 差分をとる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUND=1000\n",
    "\n",
    "for file in tqdm(glob.glob(\"../../etc/label/*.csv\")):\n",
    "    file_name=os.path.basename(file).split(\".csv\")[0]\n",
    "    new_df=pd.DataFrame()\n",
    "    label=pd.read_csv(\"../../etc/label/{}.csv\".format(file_name), header=None)\n",
    "    index=0\n",
    "            \n",
    "    df=pd.read_json(\"../../etc/no_header_pdr_raw_data/{}.json\".format(file_name), orient='records', lines=True)\n",
    "\n",
    "    print('{}LOAD DONE!!!!'.format(file_name))\n",
    "            \n",
    "    for i in range(label[0][0], label[0][len(label)-1]+1):\n",
    "        df_all=df[df['unixTime']//ROUND==((df['unixTime'].min()//ROUND)+i)]\n",
    "        df_acc=df_all[df_all['type']=='Accelerometer']\n",
    "        df_gyro=df_all[df_all['type']=='Gyroscope']\n",
    "        \n",
    "        df_old_all=df[df['unixTime']//ROUND==((df['unixTime'].min()//ROUND)+i-1)]\n",
    "        df_old_acc=df_old_all[df_old_all['type']=='Accelerometer']\n",
    "        df_old_gyro=df_old_all[df_old_all['type']=='Gyroscope']\n",
    "        if len(df_acc)==0 or len(df_gyro)==0:\n",
    "            index+=1\n",
    "            continue\n",
    "\n",
    "        old_acc_x_ave, old_acc_y_ave, old_acc_z_ave, old_acc_range, old_acc_x_std, old_acc_y_std, old_acc_z_std, old_gyro_range, \\\n",
    "        old_gyro_x_ave, old_gyro_y_ave, old_gyro_z_ave, old_gyro_x_std, old_gyro_y_std, old_gyro_z_std, old_acc_std, old_gyro_std, \\\n",
    "        old_acc_skewness, old_gyro_skewness, old_acc_kurtosis, old_gyro_kurtosis, old_acc_energy, old_gyro_energy, old_acc_ave, old_gyro_ave= \\\n",
    "        get_Ave_value(df_old_acc['x']), get_Ave_value(df_old_acc['y']), get_Ave_value(df_old_acc['z']), get_Range(df_old_acc), \\\n",
    "        get_Std_value(df_old_acc['x']), get_Std_value(df_old_acc['y']), get_Std_value(df_old_acc['z']), get_Range(df_old_gyro), \\\n",
    "        get_Ave_value(df_old_gyro['x']), get_Ave_value(df_old_gyro['y']), get_Ave_value(df_old_gyro['z']), get_Std_value(df_old_gyro['x']), \\\n",
    "        get_Std_value(df_old_gyro['y']), get_Std_value(df_old_gyro['z']), get_Std(df_old_acc), get_Std(df_old_gyro), get_Skewness(df_old_acc), \\\n",
    "        get_Skewness(df_old_gyro), get_Kurtosis(df_old_acc), get_Kurtosis(df_old_gyro), get_Energy(df_old_acc), get_Energy(df_old_gyro), get_ave(df_old_acc), get_ave(df_old_gyro)\n",
    "\n",
    "        acc_x_ave, acc_y_ave, acc_z_ave, acc_range, acc_x_std, acc_y_std, acc_z_std, gyro_range, \\\n",
    "        gyro_x_ave, gyro_y_ave, gyro_z_ave, gyro_x_std, gyro_y_std, gyro_z_std, acc_std, gyro_std, \\\n",
    "        acc_skewness, gyro_skewness, acc_kurtosis, gyro_kurtosis, acc_energy, gyro_energy, acc_ave, gyro_ave= \\\n",
    "        get_Ave_value(df_acc['x']), get_Ave_value(df_acc['y']), get_Ave_value(df_acc['z']), get_Range(df_acc), \\\n",
    "        get_Std_value(df_acc['x']), get_Std_value(df_acc['y']), get_Std_value(df_acc['z']), get_Range(df_gyro), \\\n",
    "        get_Ave_value(df_gyro['x']), get_Ave_value(df_gyro['y']), get_Ave_value(df_gyro['z']), get_Std_value(df_gyro['x']), \\\n",
    "        get_Std_value(df_gyro['y']), get_Std_value(df_gyro['z']), get_Std(df_acc), get_Std(df_gyro), get_Skewness(df_acc), \\\n",
    "        get_Skewness(df_gyro), get_Kurtosis(df_acc), get_Kurtosis(df_gyro), get_Energy(df_acc), get_Energy(df_gyro), get_ave(df_acc), get_ave(df_gyro)\n",
    "\n",
    "        new_df=new_df.append({'label':label[1][index], 'user':file_name, 'acc_x_ave':acc_x_ave, 'acc_y_ave':acc_y_ave, 'acc_z_ave':acc_z_ave, \n",
    "                              'acc_range':acc_range, 'acc_x_std':acc_x_std, 'acc_y_std':acc_y_std, 'acc_z_std':acc_z_std, 'gyro_range':gyro_range, \n",
    "                              'gyro_x_ave':gyro_x_ave, 'gyro_y_ave':gyro_y_ave, 'gyro_z_ave':gyro_z_ave, 'gyro_x_std':gyro_x_std, \n",
    "                              'gyro_y_std':gyro_y_std, 'gyro_z_std':gyro_z_std,'acc_std':acc_std, 'gyro_std':gyro_std, 'acc_skewness':acc_skewness, \n",
    "                              'gyro_skewness':gyro_skewness, 'acc_kurtosis':acc_kurtosis, 'gyro_kurtosis':gyro_kurtosis, 'acc_energy':acc_energy, \n",
    "                              'gyro_energy':gyro_energy, 'acc_ave':acc_ave, 'gyro_ave':gyro_ave, \n",
    "                              'dif_acc_x_ave':(acc_x_ave-old_acc_x_ave), 'dif_acc_y_ave':(acc_y_ave-old_acc_y_ave), 'dif_acc_z_ave':(acc_z_ave-old_acc_z_ave), \n",
    "                              'dif_acc_range':(acc_range-old_acc_range), 'dif_acc_x_std':(acc_x_std-old_acc_x_std), \n",
    "                              'dif_acc_y_std':(acc_y_std-old_acc_y_std), 'dif_acc_z_std':(acc_z_std-old_acc_z_std), \n",
    "                              'dif_gyro_range':(gyro_range-old_gyro_range), 'dif_gyro_x_ave':(gyro_x_ave-old_gyro_x_ave), 'dif_gyro_y_ave':(gyro_y_ave-old_gyro_y_ave), \n",
    "                              'dif_gyro_z_ave':(gyro_z_ave-old_gyro_z_ave), 'dif_gyro_x_std':(gyro_x_std-old_gyro_x_std), \n",
    "                              'dif_gyro_y_std':(gyro_y_std-old_gyro_y_std), 'dif_gyro_z_std':(gyro_z_std-old_gyro_z_std), 'dif_acc_std':(acc_std-old_acc_std), \n",
    "                              'dif_gyro_std':(gyro_std-old_gyro_std), 'dif_acc_skewness':(acc_skewness-old_acc_skewness), \n",
    "                              'dif_gyro_skewness':(gyro_skewness-old_gyro_skewness), 'dif_acc_kurtosis':(acc_kurtosis-old_acc_kurtosis), \n",
    "                              'dif_gyro_kurtosis':(gyro_kurtosis-old_gyro_kurtosis), 'dif_acc_energy':(acc_energy-old_acc_energy), \n",
    "                              'dif_gyro_energy':(gyro_energy-old_gyro_energy), 'dif_acc_ave':(acc_ave-old_acc_ave), 'dif_gyro_ave':(gyro_ave-old_gyro_ave)}, \n",
    "                         ignore_index=True)\n",
    "\n",
    "        index+=1\n",
    "    new_df.to_csv(\"../../etc/dif_features/{}.csv\".format(file_name), index=False)\n",
    "    print('{}DONE!!!!'.format(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# タイムウインドウ重複"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glob.glob('../../etc/label/*.csv')[66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(glob.glob(\"../../etc/label/*.csv\")):\n",
    "    file_name=os.path.basename(file).split(\".csv\")[0]\n",
    "    all_df=pd.DataFrame()\n",
    "    label=pd.read_csv(\"../../etc/label/{}.csv\".format(file_name), header=None)\n",
    "    index=0\n",
    "            \n",
    "    df=pd.read_json(\"../../etc/no_header_pdr_raw_data/{}.json\".format(file_name), orient='records', lines=True)\n",
    "\n",
    "    print('{}LOAD DONE!!!!'.format(file_name))\n",
    "    \n",
    "    for i in range(label[0][0], label[0][len(label)-1]+1):\n",
    "        new_df=pd.DataFrame()\n",
    "        new_df=new_df.append({'label':label[1][index], 'user':file_name}, ignore_index=True)\n",
    "        for window in range(0, 4):\n",
    "            features=get_features(df, i, window)\n",
    "            if len(features)==0:\n",
    "                index+=1\n",
    "                continue\n",
    "            new_df=pd.concat([new_df, features], axis=1)\n",
    "        all_df=all_df.append(new_df)\n",
    "            \n",
    "        index+=1\n",
    "    all_df.to_csv(\"../../etc/windows_features/{}.csv\".format(file_name), index=False)\n",
    "    print('{}DONE!!!!'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUND=1000\n",
    "\n",
    "for file in glob.glob(\"../../etc/label/*.csv\")[1:]:\n",
    "    file_name=os.path.basename(file).split(\".csv\")[0]\n",
    "    df=pd.DataFrame(columns=[\"x\", \"y\", \"z\", \"unixTime\", \"type\"])\n",
    "    new_df=pd.DataFrame(columns=[\"label\", \"acc_range\", \"acc_std\", \"acc_skewness\", \"acc_kurtosis\", \"acc_energy\"])\n",
    "    label=pd.read_csv(\"../../etc/label/{}.csv\".format(file_name), header=None)\n",
    "    index=0\n",
    "    is_first=True\n",
    "\n",
    "    with open(\"../../etc/pdr_raw_data/{}.json\".format(file_name), \"r\") as f:\n",
    "        lines=f.readlines()\n",
    "        for line in lines[1:]:\n",
    "            l=json.loads(line)\n",
    "            df=df.append({'x':l[\"x\"], 'y':l[\"y\"], 'z':l[\"z\"], 'unixTime':l[\"unixTime\"], 'type':l[\"type\"]}, ignore_index=True)\n",
    "\n",
    "    print('{}LOAD DONE!!!!'.format(file_name))\n",
    "    \n",
    "    for i in range(label[0][0], label[0][len(label)-1]+1):\n",
    "        if is_first==True:\n",
    "            df_acc=df[df['unixTime']//ROUND==((df['unixTime'].min()//ROUND)+i)]\n",
    "            df_acc=df_acc[df_acc['type']=='Accelerometer']\n",
    "            is_first=False\n",
    "        else:\n",
    "            df_acc=df[((df['unixTime']//ROUND)-1)==((df['unixTime'].min()//ROUND)+i-1)]\n",
    "            df_acc=df_acc.append(df[df['unixTime']//ROUND==((df['unixTime'].min()//ROUND)+i)], ignore_index=True)\n",
    "            df_acc=df_acc[df_acc['type']=='Accelerometer']\n",
    "        if len(df_acc)==0:\n",
    "            index+=1\n",
    "            continue\n",
    "        new_df=new_df.append({'label':label[1][index], 'acc_range':get_Range(df_acc), 'acc_std':get_Std(df_acc), \n",
    "                              'acc_skewness':get_Skewness(df_acc), 'acc_kurtosis':get_Kurtosis(df_acc), 'acc_energy':get_Energy(df_acc)}, \n",
    "                             ignore_index=True)\n",
    "        index+=1\n",
    "    new_df.to_csv(\"../../etc/timed_features/{}.csv\".format(file_name), index=False)\n",
    "    print('{}DONE!!!!'.format(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# raw_dataへのラベル付け"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ROUND=1000\n",
    "\n",
    "for file in glob.glob(\"../../etc/label/*.csv\"):\n",
    "    file_name=os.path.basename(file).split(\".csv\")[0]\n",
    "    new_df=pd.DataFrame()\n",
    "    label=pd.read_csv(\"../../etc/label/{}.csv\".format(file_name), header=None)\n",
    "    index=0\n",
    "    \n",
    "    df=pd.read_json(\"../../etc/no_header_pdr_raw_data/{}.json\".format(file_name), orient='records', lines=True)\n",
    "\n",
    "    print('{}LOAD DONE!!!!'.format(file_name))\n",
    "            \n",
    "    for i in range(label[0][0], label[0][len(label)-1]+1):\n",
    "        df_acc=df[df['unixTime']//ROUND==((df['unixTime'].min()//ROUND)+i)]\n",
    "        # df_acc=df_acc[df_acc['type']=='Accelerometer']\n",
    "        if len(df_acc)==0:\n",
    "            index+=1\n",
    "            continue\n",
    "        df_acc['user']=file_name\n",
    "        df_acc['label']=label[1][index]\n",
    "        new_df=new_df.append(df_acc, ignore_index=True)\n",
    "        index+=1\n",
    "    new_df.to_csv(\"../../etc/labeled_pdr_raw_data/{}.csv\".format(file_name), index=False)\n",
    "    print('{}DONE!!!!'.format(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ作るぞ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob(\"../../etc/test/*.csv\"):\n",
    "    file_name=os.path.basename(file).split(\".csv\")[0]\n",
    "    df_features=pd.read_csv(\"../../etc/test/{}.csv\".format(file_name))\n",
    "    df_features['user']=file_name\n",
    "    df_features.to_csv(\"../../etc/test/{}.csv\".format(file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awk 'NR==1 || FNR!=1' *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29360\n",
      "29360\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('../../etc/step_num/features.csv')\n",
    "print(len(df))\n",
    "dfs=df.drop(['label', 'user'], axis=1)\n",
    "\n",
    "for column in dfs.columns:\n",
    "    p01=dfs[column].quantile(0.01)\n",
    "    p99=dfs[column].quantile(0.99)\n",
    "    dfs[column]=dfs[column].clip(p01, p99)\n",
    "print(len(dfs))\n",
    "# print(max(dfs['acc_ave']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['user']=df['user']\n",
    "dfs['label']=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../etc/step_num/clipped_raw_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features=pd.read_csv('../../etc/step_num/clipped_raw_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features=df_features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SC', 'label', 'user'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=df_features.columns[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "\n",
    "for column in columns:\n",
    "    # print(stats.zscore(df_features[column].values))\n",
    "    df_features[column]=stats.zscore(df_features[column].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.to_csv('../../etc/step_num/clipped_std_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "mm_acc=minmax.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=pd.DataFrame()\n",
    "\n",
    "for row in df_features.itertuples():\n",
    "    new_df=new_df.append({'label':row[14], 'user':row[15], 'acc_range':std_acc[row[0]][3], 'acc_std':std_acc[row[0]][5], \n",
    "                          'acc_skewness':std_acc[row[0]][4], 'acc_kurtosis':std_acc[row[0]][2], 'acc_energy':std_acc[row[0]][1], \n",
    "                          'gyro_range':std_acc[row[0]][9], 'gyro_std':std_acc[row[0]][11], 'gyro_skewness':std_acc[row[0]][10], \n",
    "                          'gyro_kurtosis':std_acc[row[0]][8], 'gyro_energy':std_acc[row[0]][7], 'acc_ave':std_acc[row[0]][0], \n",
    "                          'gyro_ave':std_acc[row[0]][6]},  \n",
    "                             ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('../../etc/labeled_features/clipped_std_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 時刻t-1のラベルを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features=pd.read_csv('../../etc/dif_features/clipped_std_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "new_df=pd.DataFrame()\n",
    "\n",
    "for file in glob.glob(\"../../etc/label/*.csv\"):\n",
    "    labels=[]\n",
    "    file_name=os.path.basename(file).split(\".csv\")[0]\n",
    "    df=df_features[df_features['user']==file_name]\n",
    "    labels+=[list(df['label'])[0]]\n",
    "    labels+=list(df['label'])[:len(df)-1]\n",
    "    df['pre_label']=labels\n",
    "    new_df=new_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('../../etc/dif_features/clipped_std_prelabel_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../../etc/windows_features/clipped_std_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=df.drop(['label', 'user'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(features)\n",
    "features = pca.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(features, columns=[\"PC{}\".format(x + 1) for x in range(len(df.drop(['label', 'user'], axis=1).columns))]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(features, columns=[\"PC{}\".format(x + 1) for x in range(len(df.drop(['label', 'user'], axis=1).columns))]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 累積寄与率を図示する\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "plt.gca().get_xaxis().set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "plt.plot([0] + list( np.cumsum(pca.explained_variance_ratio_)), \"-o\")\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.ylabel(\"Cumulative contribution rate\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=pd.DataFrame(features, columns=[\"PC{}\".format(x + 1) for x in range(len(df.drop(['label', 'user'], axis=1).columns))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=new_df.iloc[:, 0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "# 主成分数を指定して、PCA のインスタンスを生成\n",
    "pca = PCA()\n",
    "# ロジスティック回帰のインスタンスを生成\n",
    "lr = LogisticRegression()\n",
    "# トレーニングデータとテストデータで PCA を実行\n",
    "# X_train_pca = pca.fit_transform(features)\n",
    "# X_test_pca = pca.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('../../etc/windows_features/pca.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
